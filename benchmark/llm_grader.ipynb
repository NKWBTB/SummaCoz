{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gluo/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.20s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "MODEL = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            trust_remote_code=True,\n",
    "                                            device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_jsonl\n",
    "\n",
    "gt = load_jsonl(\"benchmark_v_0_5.jsonl\")\n",
    "pred = load_jsonl(\"llama2-13b-posthoc.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \\\n",
    "\"\"\"<s>[INST] You are an assessor to give judgement on a reasoning problem.\n",
    "Here is the text to be assessed:\n",
    "\n",
    "<text>\n",
    "{input_reasoning}\n",
    "</text>\n",
    "\n",
    "Does the above text mention or contain the following reference reasoning step:\n",
    "\n",
    "<reference>\n",
    "{reference}\n",
    "</reference>\n",
    "\n",
    "Answer (yes or no):[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(input_txt):\n",
    "    sequences = pipeline(\n",
    "            input_txt,\n",
    "            max_new_tokens=16,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            return_full_text=False\n",
    "    )\n",
    "    return sequences[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary mentions five ambitious clubs are locked in a bid for two champions league places.\n",
      "But in the article it never mentions the word 'bid', the truth in article is five ambitious clubs are locked in a scramble for two Champions League places.\n",
      "As the summary has the word should not be contained, it is inconsistent with article.\n",
      "Therefore, the answer is no, the summary is not consistent with the article.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize\n",
    "def process_human_reasoning(human_reason):\n",
    "    human_reason = re.sub(r'\\n+', '\\n', human_reason).strip()\n",
    "    # items = sent_tokenize(human_reason)\n",
    "    items = human_reason.split(\"\\n\")\n",
    "    items = [re.sub(r'^\\d+\\.\\s*', '', item).strip() for item in items]\n",
    "    # items = [item for item in items if len(item) > 0]\n",
    "    return items\n",
    "print(\"\\n\".join(process_human_reasoning(gt[1][\"human_reason\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gluo/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/gluo/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No, the text does not mention or contain the reference reasoning step you provided\n"
     ]
    }
   ],
   "source": [
    "sample1 = template.format(input_reasoning=pred[1][\"reason\"], reference=process_human_reasoning(gt[1][\"human_reason\"])[1])\n",
    "\n",
    "print(llm_generate(sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def swap_punctuation(input:str):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return input.translate(translator)\n",
    "\n",
    "def answer_parse(input:str):\n",
    "    words = swap_punctuation(input.lower()).split()\n",
    "    try: \n",
    "        assert (\"not\" in words) or (\"no\" in words) or (\"yes\" in words)\n",
    "    except:\n",
    "        print(words)\n",
    "\n",
    "    return 0 if (\"no\" in words) or (\"not\" in words) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1268/1330 [59:23<02:54,  2.81s/it] \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/gluo/FactInstruct/benchmark/llm_grader.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m ground_truth:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     answer \u001b[39m=\u001b[39m llm_generate(template\u001b[39m.\u001b[39mformat(input_reasoning\u001b[39m=\u001b[39mpredicted, reference\u001b[39m=\u001b[39mitem))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     points\u001b[39m.\u001b[39mappend(answer_parse(answer))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print(points)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(points)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(points))\n",
      "\u001b[1;32m/home/gluo/FactInstruct/benchmark/llm_grader.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manswer_parse\u001b[39m(\u001b[39minput\u001b[39m:\u001b[39mstr\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     words \u001b[39m=\u001b[39m swap_punctuation(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mlower())\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m words) \u001b[39mor\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m words) \u001b[39mor\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m words)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshanghai.tail81053.ts.net/home/gluo/FactInstruct/benchmark/llm_grader.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m words) \u001b[39mor\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m words) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "scores = []\n",
    "for idx, human in enumerate(tqdm(gt[1266:])):\n",
    "    predicted = pred[idx][\"reason\"]\n",
    "    # print(predicted)\n",
    "    ground_truth = process_human_reasoning(human[\"human_reason\"])\n",
    "    points = []\n",
    "    for item in ground_truth:\n",
    "        answer = llm_generate(template.format(input_reasoning=predicted, reference=item))\n",
    "        points.append(answer_parse(answer))\n",
    "    # print(points)\n",
    "    scores.append(sum(points)/len(points))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
